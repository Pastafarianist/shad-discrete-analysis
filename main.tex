\documentclass[12pt]{article}
\usepackage{fontspec}

\usepackage[cm]{fullpage}
\usepackage{mathtools,amssymb}
\usepackage{paralist}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}

\setmainfont{Liberation Serif}
% \pagenumbering{gobble}

\DeclarePairedDelimiter\autobracket{(}{)}
\newcommand{\br}[1]{\autobracket*{#1}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathbb{P}}

\title{Конспект по дискретному анализу}

\begin{document}

\maketitle

Самая свежая версия конспекта находится по ссылке:

\begin{center}
\href{https://www.overleaf.com/read/ktvkmfxfgdfk}{https://www.overleaf.com/read/ktvkmfxfgdfk}
\end{center}

На гитхабе находится копия, там принимаются пулл-реквесты: \href{https://github.com/Pastafarianist/shad-discrete-analysis}{https://github.com/Pastafarianist/shad-discrete-analysis}. Также с автором можно связаться в телеграме (\href{https://t.me/Erring}{\texttt{@Erring}}).

\section{2017-09-07: лекция 1}

\subsection{}

Пусть $f \colon \mathbb R \to \mathbb R$, $g \colon \mathbb R \to \mathbb R$.

Будем называть $f$ и $g$ эквивалентными ($f \sim g$), если НСНМ $f(x) = (1 + o(1)) g(x)$.

(Хотелось бы написать "$\frac {f(x)} {g(x)} \to 1$", но это не вполне корректно, если $g$ обращается в $0$.)

Рассмотрим формулу:

\[
\sum_{k=0}^n \left( C^k_n \right)^4
\]

Эта формула вылезает из теории графов и теорвера, и иногда хочется уметь вычислять её асимптотику.

Обозначим за $G(n, r, s)$ граф, в котором вершины — это $r$-элементные подмножества $\{1 \ldots n\}$, а рёбра проведены между вершинами, если множества, соответствующие вершинами, пересекаются ровно по $s$ элементам. Легко показать, что $|V| = C^r_n$, $|E| = \frac 1 2 C^r_n  C^s_r C^{r - s}_{n_r}$. Проведём замену:

\begin{align*}
n &\to 4n \\
r &\to 2n \\
s &\to n
\end{align*}

Тогда $|E| = \frac 1 2 C^{2n}_{4n} C^n_{2n} C^n_{2n}$. Рассмотрим задачу вычисления количества треугольников в этом графе. Будем подступаться к ней так: сначала рассмотрим ребро, соединяющее две вершины (два $2n$-элементных подмножества, пересекающихся по $n$ элементов) и посчитаем количество третьих вершин, которые соединены с этими двумя одновременно. Если третья вершина соединена с двумя другими, то её множество пересекается с пересечением размера $n$ по $k$ элементов; с разностью первого множества и пересечения по $n-k$ элементов; с разностью второго множества и пересечения по $n-k$ элементов; с дополнением обоих множеств по $k$ элементов. Итого:

\[ \Delta = C_n^k C_n^{n-k} C_n^{n-k} C_n^k \]

А всего треугольников:

\[ \frac 1 6 |E| \Delta \]

\subsection{}

Попробуем оценить какие-нибудь простые асимптотики.

\begin{align*}
C^n_{2n} &< 2^{2n} = \sum_{k=0}^{2n} C^k_{2n} \\
C^n_{2n} &> \frac {2^{2n}} {2n + 1} = \frac {\sum_{k=0}^{2n} C^k_{2n}} {2n + 1} \\
\end{align*}

Это позволяет оценить асимпотику логарифма:

\begin{align*}
\ln C^n_{2n} &< 2n \ln 2 \\
\ln C^n_{2n} &> 2n \ln 2 - \ln(2n + 1) \\
\Rightarrow \ln C^n_{2n} &\sim 2n \ln 2 \\
\end{align*}

По сути, это оценка на асимптотику логарифма $C^{n/2}_n$. Хотелось бы оценить асимптотику логарифма $C^{[an]}_n$. Теорема:

\[ C^{[an]}_n = \left( \frac 1 {a^a (1-a)^{1-a}} + o(1) \right)^n \]

Напоминание: формула Стирлинга.

\[ n! \sim \sqrt{2 \pi n} \left( \frac n e \right)^n \]

(Теорему можно доказать с её помощью.)

За оставшееся время получим несколько асимптотик на $C^k_n$.

\[ C^k_n = \frac {n!} {k! (n-k)!} = \frac {n (n-1) \ldots (n - k + 1)} {k!}  < \frac {n^k} {k!} \]

А также:

\begin{multline*}
\frac {n (n-1) \ldots (n - k + 1)} {k!} = \\
\frac {n^k} {k!} \left(1 - \frac 1 n \right) \left(1 - \frac 2 n \right) \ldots \left(1 - \frac {k-1} n \right) = \\
\frac {n^k} {k!} \exp \left( \ln\left(1 - \frac 1 n \right) + \ln \left(1 - \frac 2 n \right) + \ldots + \ln \left(1 - \frac {k-1} n \right) \right) = \\
\frac {n^k} {k!} \exp \left( - \frac 1 n - \frac 2 n - \ldots - \frac {k-1} n + O\left(\frac {k^3} {n^2}\right) \right) = \\
\frac {n^k} {k!} \exp \left( - \frac {k (k-1)} {2n} + O \left(\frac {k^3} {n^2} \right) \right)
\end{multline*}

Отсюда при $k = O(\sqrt{n})$ асимптотика $C^k_n \sim \frac {n^k} {k!}$ точная.

\section{2017-09-14: лекция 2}

\subsection{}

Как пересчитать все деревья на n вершинах с пронумерованными вершинами?

Ответ будет $n^{n-2}$. Для того, чтобы его получить, рассмотрим все последовательности длиной $n-2$, в которых бывают числа от $1$ до $n$, и построим между такими последовательностями и деревьями биекцию. (google://коды Прюфера)

\begin{itemize}
\item Отображение дерево → последовательность:

Найдём в дереве лист (вершину степени 1) с минимальным индексом и уберём её из графа вместе с единственным выходящим из неё ребром. Запишем, в какую вершину оно вело. Будем повторять это обрезание, пока в графе не останутся всего 2 вершины. Получим искомую последовательность.

\item Отображение последовательность → дерево:

Выписываем рядом с последовательностью подряд все числа от $1$ до $n$. Их на два больше, поэтому там найдётся число, которого нет в последовательности. Добавим в граф ребро с наименьшим таким числом и первым числом из последовательности. Вычеркнем оба использованных числа. Будем продолжать, пока в наборе чисел от $1$ до $n$ не останется всего два числа; тогда проведём ребро между этими двумя числами.
\end{itemize}

\subsection{}

Определение: унициклический граф — это связный граф, в котором ровно один цикл (т.е. на $n$ вершинах $n$ рёбер).

Хотим, разумеется, их пересчитать.

Переберём все различные возможные длины цикла $r$, для каждой длины выберем $r$ из $n$ вершин и посадим на них цикл, затем цикл пронумеруем всеми возможными способами, и наконец на вершины цикла посадим лес, воспользовавшись формулой для количества лесов с $r$ деревьями, в которых вершины $1, 2, \ldots r$ принадлежат разным деревьям (её мы, вероятно, разберём на семинаре):

\[
\mathcal{U}_n = \sum_{r = 3}^n C^r_n \frac {r!} {2 r} r n^{n - 1 - r}
\]

Интересен асимптотический рост: во сколько раз это отличается от $n^{n - 2}$?

\begin{align*}
C^r_n \frac {r!} 2 n^{n - 1 - r} &= \frac {n!} {r!(n-r)!} \frac {r!} 2 n^{n-1-r} \\
&= n^r \left(1 - \frac 1 n \right) \ldots \left(1 - \frac {r - 1} n \right) \frac {n^{n - 1 - r}} 2 \\
&= \frac 1 2 n^{n - 1} \left(1 - \frac 1 n\right) \ldots \left(1 - \frac {r - 1} n\right) \\
&= \frac 1 2 n^{n - 1} \exp\left(\ln\left(1 - \frac 1 n\right) + \ldots + \ln\left(1 - \frac {r - 1} n\right)\right) \\
\end{align*}

Вспомним, что $\ln(1 - x) \leq -x$, и кроме того $\ln(1 - x) = -x + O(x^2)$. Получаем оценку:

\[
\ldots \leq \exp \left( -\frac {r(r-1)} {2n} \right)
\]

и

\[
\ldots = \exp \left( -\frac {r(r-1)} {2n} + O \left(\frac {r^3} {n^2} \right) \right)
\]

Порежем промежуток суммирования по чему-то между $\frac 1 2$ и $\frac 2 3$, например, $0.6$.

Сумма от $\left\lfloor n^{0.6} \right\rfloor$ до $n$ оценивается неравенством с экспонентой и с запасом асимптотически улетает в 0.

Сумма от $3$ до $\left\lfloor n^{0.6} \right\rfloor$ оценивается суммой с такими же пределами от $\exp \left( -\frac {r^2} {2n} \right)$. Сумму можно оценить интегралом, интеграл легко считается.

Итоговый ответ: $\sqrt{\frac \pi 8} n^{n - \frac 1 2}$.

\section{2017-09-28: лекция 3}

\subsection{Обходы графов}

Определение: граф называется эйлеровым, если есть цикл, который проходит по всем рёбрам.

Для связного графа следующие 3 утверждения эквивалентны:

\begin{enumerate}
\item Граф эйлеров
\item Степень каждой вершины чётная
\item Множество рёбер можно разбить на непересекающиеся по рёбрам простые циклы
\end{enumerate}

Определение: граф называется гамильтоновым, если существует простой цикл, проходящий по всем вершинам (простой = посещающий каждую вершину только один раз).

Признак Дирака: если $|V| = n$ и $\forall v \colon \operatorname{deg} v \geq \frac n 2$, то он гамильтонов.

Признак Эрдёша-Хвáтала: сначала определим два вспомогательных числа. "Число независимости" графа $\alpha (G)$ — это размер максимального независимого множества (или максимальной "антиклики"), то есть

\[
\alpha(G) = \max \{ k \colon \exists W \subset V \colon |V| = k, \forall x, y \in W \colon (x, y) \not\in E \}
\]

"Вершинная связность" графа $\kappa (G)$ — это размер наименьшего множества вершин, при удалении которого из графа он перестаёт быть связным, то есть

\[
\kappa (G) = \min \{ k \colon \exists W \subset V \colon |W| = k, G|_{V \setminus W} \text{ — не связен} \}
\]

Признак формулируется так. Пусть $|V| \geq 3$ и $\alpha(G) \leq \kappa(G)$. Тогда $G$ гамильтонов. Докажем это.

Предположим противное. Пусть $\alpha(G) \leq \kappa(G)$, но граф не гамильтонов. Заметим, что такой граф в любом случае связен (если он не связен, то $\kappa(G) = 0$, но $\alpha(G) \geq 2$). Рассмотрим два случая.

\begin{enumerate}
\item В графе нет циклов, то есть он дерево. Тогда $\kappa(G) = 1, \alpha(G) \geq 2$.
\item Цикл(ы) есть. Возьмём самый длинный простой цикл $C = \{ x_1, \ldots x_k \}$, $k < n := |V|$ (если $k = n$, то граф состоит из одного цикла и гамильтонов). Пусть $G'$ — это граф, получаемый из $G$ удалением $C$, и рассмотрим любую связную компоненту $W$ графа $G'$. Обозначим

\[ N_W(G) = \{ v \in V \setminus W \colon \exists w \in W \colon (v, w) \in E \} \]

\begin{itemize}
\item Утверждение: $N_W(G) \subset C$. Идея доказательства: нужно рассмотреть разбиение графа $G$ на $C$, $W$ и всё остальное.
\item Утверждение: если $x_i \in N_W(G)$, то $x_{i+1} \not\in N_W(G)$ (то есть соседние вершины на цикле $C$ не могут одновременно лежать в $N_W(G)$). Отсюда $N_W(G) \neq C$.
\item Утверждение: $\kappa(G) \leq |N_W(G)|$, потому что если удалить из графа все вершины из $N_W(G)$, $W$ станет отдельной компонентой связности.
\end{itemize}

Обозначим $M = \{ x_{i+1} \colon x_i \in N_W(G) \}$. Тогда $M \cap N_W(G) = \emptyset$, $|M| = |N_W(G)|$.

Утверждение: $M$ — независимое множество. Предположим противное: $\exists i, j \colon x_{i + 1}, x_{j + 1} \in M$ и $x_{i + 1}, x_{j + 1} \in E$. Обозначим $a, b \in W$ — соседи вершин $x_i, x_j$ в $W$. Тогда между $x_i$ и $x_{j + 1}$ можно вклеить следующий кусок: $x_i \to a \to b \to x_j \to \ldots \to x_{i + 1} \to x_{j + 1}$, в котором исчезли два ребра и добавились три, что даёт цикл на единицу более длинный, чем $C$ и противоречит предположению о том, что $C$ — самый длинный цикл. Поэтому $M$ — независимое множество.

Получили: $\kappa(G) \leq |N_W(G)| = |M| \leq \alpha(G)$. Но заметим, что если к $M$ добавить любую вершину из $W$, оно не перестанет быть независимым, и поэтому $\kappa(G) < \alpha(G)$, что противоречит изначальному предположению.
\end{enumerate}

\section{2017-10-12: лекция 4}

Определение: говорим, что последовательность $\{ y_n \}_{n=1}^\infty$ удовлетворяет линейному рекуррентному соотношению $k$-го порядка, с постоянными коэффициентами $a_0, a_1, \ldots, a_k$, если $\forall n \colon a_k y_{n + k} + a_{k - 1} y_{k - 1} + \ldots + a_1 y_{n + 1} + a_0 y_n$. Будем считать, что $a_k, a_0 \neq 0$; тогда $y_{n + k}$ выражается через $k$ предыдущих элементов последовательности.

Рассмотрим случай $k = 2$, $a_2 y_{n + 2} + a_1 y_{n + 1} + a_0 y_n = 0$. Алгоритм поиска явного решения: выписываем характеристическое уравнение.

\[
a_2 x^2 + a_1 x + a_0 = 0
\]

У него могут быть 2 вещественных решения, 2 комплексных решения или 1 кратное решение. В любом случае левая часть уравнения переписывается как $a_2 (x - \lambda_1) (x - \lambda_2)$.

\textbf{Теорема:}

\begin{enumerate}
\item
    Пусть $\lambda_1 \neq \lambda_2$. Тогда
    \begin{enumerate}
    \item $\forall c_1, c_2 \colon y_n = c_1 \lambda_1^n + c_2 \lambda_2^n$ удовлетворяет рекуррентному соотношению (в дальнейшем обозначаем его (*))
    \item Если $\{ y_n \}$ удовлетворяет (*), то $\exists c_1, c_2 \colon y_n = c_1 \lambda_1^n + c_2 \lambda_2^n$.
    \end{enumerate}
\item
    Если $\lambda_1 = \lambda_2$, то $y_n = (c_1 n + c_2) \lambda_1^n$, а остальные утверждения из первого пункта не изменяются.
\end{enumerate}


\textbf{Доказательство:}
\begin{enumerate}
\item
\begin{enumerate}
\item Подставляем и проверяем.
\item Решаем систему уравнений:

\[
\begin{cases}
c_1 + c_2 = y_0 \\
c_1 \lambda_1 + c_2 \lambda_2 = y_1
\end{cases}
\]

получаем решение $(c_1^*, c_2^*)$, строим последовательность $y_n^* = c_1^* \lambda_1^n + c_2^* \lambda_2^n$, проверяем, что $y_0^* = y_0$, $y_1^* = y_1$ и замечаем, что первые два члена однозначно определяют последовательность (в силу рекуррентного соотношения), поэтому это искомая последовательность.
\end{enumerate}
\item Аналогично.
\end{enumerate}

Теперь рассмотрим то же самое в общем случае.

\begin{itemize}
\item Рекуррентная формула: $a_k y_{n + k} + a_{k - 1} y_{k - 1} + \ldots + a_1 y_{n + 1} + a_0 y_n$
\item Характеристическое уравнение: $a_k x^k + \ldots + a_1 x + a_0 = 0$
\item Его решение: $a_k(x - \lambda_1) (x - \lambda_2) \cdots (x - \lambda_k)$
\end{itemize}

Некоторые из $\lambda_i$ могут повторяться; обозначим все различные числа за $\mu_1, \ldots, \mu_r$ и их кратность за $m_1, \ldots m_r$. Тогда

\[
y_n = P_{m_1 - 1}(n) \mu_1^n + \cdots + P_{m_r - 1}(n) \mu_r^n
\]

где $P_d(n)$ — многочлены степени $d$ от $n$.

<пропущено интуитивное объяснение общего случая>

\subsection{Формальные степенные ряды}

Определяем формальные ряды как бесконечные последовательности чисел с операциями на них. Сложение и умножение понятно, деление определяем как умножение на обратное: $\frac A B = B^{-1} A = C$, если $A = BC$. Искать $B^{-1}$ можно так: написать уравнения для коэффициентов в выражении $A = BC$ и решить их относительно $(c_0, c_1, \ldots )$. Это реализует алгоритм деления в столбик.

(Очевидно, что над кольцом это коммутативное кольцо с единицей; \href{http://stu.sernam.ru/book\_t\_galua.php?id=38}{тут} говорят, что над полем это поле.)

Дальше повычисляем что-нибудь.

\[
\frac 1 {(1 - x^2)^2} = \left( \frac 1 {1 - x^2} \right)^2 = (1 + x^2 + x^4 + \ldots + x^{2n} + \ldots)^2 = 1 + 2x^2 + 3x^4 + \ldots + (n + 1) x^{2n} + \ldots
\]

Но также

\[
\frac 1 {(1 - x^2)^2} = \left( \frac 1 {1 - x} \right)^2 \left( \frac 1 {1 + x} \right)^2
\]

и отсюда можно вытащить какое-то довольно стрёмное тождество, приравняв коэффициенты при $x^n$.

За оставшееся время посчитаем числа Каталана.

\[
T_n = T_0 T_{n - 1} + T_1 T_{n - 2} + \ldots + T_{n - 2} T_1 + T_{n - 1} T_0, \text{ где } T_0 = 1
\]

Составляем степенной ряд:

\[
f(x) := T_0 + T_1 x + T_2 x^2 + \ldots
\]

Тогда

\[
f^2(x) = T_0^2 + (T_0 T_1 + T_1 T_0) x + (T_0 T_2 + T_1^2 + T_2 T_0)^2 x^2 + \ldots = T_1 + T_2 x + T_3 x^2 + \ldots
\]

и

\[
x f^2(x) = f(x) - T_0 = f(x) - 1
\]

Решаем квадратное уравнение, получаем (один корень отбрасываем из соображений на $f(0)$):

\[
x f(x) = \frac {1 - \sqrt{1 - 4x}} 2
\]

Слева при $x^n$ стоит коэффициент $T_{n - 1}$. Распишем числитель дроби справа:

\[
\sqrt{1 + x} = (1 + x)^{\frac 1 2} = 1 + C^1_{\frac 1 2} x + C^2_{\frac 1 2} x^2 + \ldots
\]

где $C^k_{\frac 1 2} = \frac {\frac 1 2 \left( \frac 1 2 - 1 \right) \cdots \left( \frac 1 2 - k + 1 \right)} {k!}$ (аналогично выражению для целых $n$). Для проверки достаточно формально возвести в квадрат ряд и проверить, что получается $1 + x$.

Утверждается, что коэффициент при $x^n$ справа будет

\begin{multline*}
- \frac 1 2 (-4)^n C^n_{\frac 1 2} =
- \frac 1 2 (-4)^n \frac {(-1)(-3) \cdots (-(2n - 3))} {2^n n!} = \\
\frac {- \frac 1 2 (-4)^n (-1)^{n - 1}} {2^n n!} \cdot \frac {(2n - 2)!} {2^{n-1} (n - 1)!} = \frac {(2n - 2)!} {n! (n - 1)!} =
\frac {(2n - 2)!} {((n - 1)!)^2 \cdot n}
\end{multline*}

и отсюда $T_n = \frac {C^n_{2n}} {n + 1}$.

\section{2017-10-19: лекция 5}

Вспоминаем старую схему для построения графов $G(n, 3, 1)$:

\begin{itemize}
\item Рассматриваем множество $\{ 1 \ldots n \}$;
\item Вершины: $V := \{ A \subset \{ 1 \ldots n \} \colon |A| = 3 \}$;
\item Рёбра: $E := \{ (A, B) \colon |A \cap B| = 1 \}$
\end{itemize}

Ещё вспоминаем, что

\[
\alpha(G(n, 3, 1)) =
\begin{cases}
n, & n \equiv 0 \pmod 4 \\
n-1, & n \equiv 1 \pmod 4 \\
n-2, & n \equiv 2 \pmod 4 \\
\end{cases}
\]

\textbf{Теорема:} $\alpha(G(n, 3, 1)) \leq n$.

\textbf{Доказательство.} Заметим, что $G(n, 3, 1)$ можно описать по-другому:

\begin{align*}
V &= \{ \vec x = (x_1, \ldots, x_n) \colon x_i \in \{ 0, 1 \}, x_1 + \ldots x_n = 3 \} \\
E &= \{ \{ \vec x_1, \vec x_2 \} \colon (x_1, x_2) = 1 \}
\end{align*}

Теперь заметим, что можно считать, что $\vec x \in \mathbb{Z}_2^n$. Это векторное пространство над $\mathbb{Z}_2$ размерности $n$. В нём есть базис (линейно независимое множество) размерности $n$.

Обозначим $W = \{ \vec x_1 , \ldots , \vec x_s \}$ — независимое множество вершин в графе. Заметим, что различные вершины независимы тогда и только тогда, когда их скалярное произведение равно $1$ (т.к. оно может быть $0$, $1$ или $2 \equiv 0 \pmod 2$.

Из этого следует, что набор векторов $\vec x_1, \ldots, \vec x_s$ линейно независим, (доказательство от противного: пусть $c_1 \vec x_1 + \ldots + c_s \vec x_s \equiv \vec 0 \pmod 2$, тогда домножаем скалярно на $x_i$ и получаем, что $c_i \equiv 0 \pmod 2$).

Размер линейно независимого множества не превышает $n$. Теорема доказана.

\textbf{Теорема:} $\alpha(G(n, 5, 2)) \leq C^2_n + 2 C^1_n \sim \frac {n^2} 2$

\textbf{Замечание:} $\alpha(G(n, 5, 2)) \geq C^2_{n-3} \sim \frac {n^2} 2$ (потому что можно зафиксировать 3 какие-нибудь числа и перебирать 2 оставшихся, тогда у таких вершин пересечение будет по крайней мере 3).

\textbf{Доказательство теоремы.} Применим трюк. Рассмотрим некоторое пространство многочленов от нескольких переменных, найдём его размерность и воспользуемся фактом о том, что размер линейно независимого множества не превышает размерность пространства.

Пусть $W = \{ \vec x_1 , \ldots, \vec x_s \}$ — независимое множество вершин в графе, т.е. $\forall i, j, i \neq j \colon (\vec x_i, \vec x_j) \neq 2$. Сопоставим каждому $\vec x_i$ следующий многочлен:

\[
P_{\vec x_i}(\vec y) := (\vec x_i, \vec y) ((\vec x_i, \vec y) - 1) \in \mathbb{Z}_3 [y_1, \ldots, y_n]
\]

Дальше делаем то же, что в предыдущей теореме. Пусть $c_1 P_{\vec x_1} + \ldots + c_s P_{\vec x_s} = 0$, где $0$ означает нулевой многочлен, то есть многочлен, у которого все коэффициенты нулевые. Поэтому

\[
c_1 P_{\vec x_1}(\vec y) + \ldots + c_s P_{\vec x_s}(\vec y) \equiv 0 (mod\ 3), \quad \vec y \in W
\]

то есть при $\vec y = \vec x_i$

\begin{align*}
P_{\vec x_i}(\vec x_i) &\equiv 20 \equiv 2 (mod\ 3) \\
P_{\vec x_j}(\vec x_i) &\equiv 0 (mod\ 3) \\
\end{align*}

и отсюда $c_i \equiv 0 (mod\ 3)$.

Теперь осталось посчитать размерность базиса пространства, порождающего эти многочлены. Для этого нужно выписать произвольный вектор $\vec y$, содержащий ровно 5 единиц, подставить его в общую формулу для $P_{\vec x_i}$, раскрыть скобки и посчитать количество способов выбрать такие слагаемые. Получится как раз формула из условия теоремы. Теорема доказана.

\textbf{Замечание:} поскольку нас интересуют значения многочленов $P_{\vec x_i}$ только на векторах с элементами из нулей и единиц, а $0^2 = 0$ и $1^2 = 1$, можно заменить многочлены $P_{\vec x_i}$ на многочлены $\tilde{P}_{\vec x_i}$, в которых все квадраты заменены на первые степени. Это преобразование позволяет улучшить оценку до $C^2_n$ (убрать $C^1_n$).

Сформулируем также общую \textbf{теорему (Франкл-Уилсон, 1981)}. Пусть $p$ — простое число, $r - s = p$, $r - 2p < 0$. Тогда $\alpha(G(n, r, s)) \leq \sum_{k = 0}^{p - 1} C^k_n$. Схема её доказательства такая. Нужно ввести следующие многочлены:

\[
\vec x_i \to P_{\vec x_i}(\vec y) = \prod_{j \in \{ 1, \ldots, p \} \setminus \{ s \}} (j - (\vec x_i, \vec y)) \in \mathbb{Z}_p[\vec y]
\]

Обозначить $W = \{ \vec x_1, \ldots, \vec x_s \}$, предположить, что $\forall i, j\ \colon (\vec x_i, \vec x_j) \neq s$. Преобразовать $P_{\vec x_i}$ в $\tilde{P}_{\vec x_j}$, раскрыв скобки в $P_{\vec x_i}$, убрав все степени во всех одночленах (можно в силу идемпотентности нуля и единицы) и приведя подобные слагаемые. Записать

\[
c_1 \tilde{P}_{\vec x_1}(\vec y) + \ldots + c_s \tilde{P}_{\vec x_s}(\vec y) \equiv 0 (mod\ p)
\]

Значение этого выражения совпадает со значением выражения без тильд. Подставляем туда $\vec y = \vec x_i$. По определению можно вычислить, что $P_{\vec x_i} \not\equiv 0 (mod\ p)$.

<skip>

\section{2017-10-26: лекция 1 по линейной алгебре}

\subsection{Векторные и матричные нормы}

Пусть $V$ — векторное пространство (над $\R$ или над $\C$).

Определение: векторная норма — это отображение $||\bullet|| \colon V \to \R$, удовлетворяющее трём свойствам:

\begin{enumerate}
\item Невырожденность: $\forall v \in V \colon ||v|| = 0 \Leftrightarrow v = 0$
\item Вынесение скаляра: $\forall v \in V \forall \lambda \colon ||\lambda v|| = |\lambda| ||v||$
\item Неравенство треугольника
\item (для матричных норм) $\forall A, B \in M_n \colon ||AB|| \leq ||A|| \cdot ||B||$
\end{enumerate}

Примеры:

\begin{itemize}
\item Евклидова норма ($L_2$): $||(x_1, \ldots, x_n)||_2 = \sqrt{x_1^2 + \ldots + x_n^2}$
\item $L_1$: $||(x_1, \ldots, x_n)||_1 = |x_1|^2 + \ldots + |x_n|^2$
\item $L_\infty$: $||(x_1, \ldots, x_n)||_\infty = \max_i |x_i|$
\end{itemize}

Научимся теперь делать из векторных норм нормы линейных операторов. Пусть $V$ — векторное пространство с нормой $||\bullet||$. Обозначим $M_n = M(V)$ — множество матриц (размера $n \times n$) над $V$. Введём такую норму на $M_n$:

\[
||A|| = \max_{||v|| = 1} ||Av|| = \max_{v \neq 0} \frac {||Av||} {||v||}
\]

Из такого определения очевидно, что $||E|| = 1$ (норма единичной матрицы) и $||Av|| \leq ||A|| \cdot ||v||$.

Дальше будем всегда считать, что нормы матриц и векторов согласованы именно таким образом. Посмотрим, какой матричной норме соответствует $L_1$-норма векторов. Можно напрямую посчитать, что

\[
||(a_{ij})||_1 = \max_{i = 1, \ldots, n} (|a_{1i}| + \ldots + |a_{ni}|)
\]

то есть максимум достигается на одном из базисных векторов. Разберёмся, почему. Пусть, например, он достигается на $e_1 = (a_{11}, \ldots, a_{n1})$. Рассмотрим некоторый другой базисный вектор $e_2 = (a_{12}, \ldots, a_{n2})$. Тогда

\[
||A(\lambda e_1 + \mu e_2)||_1 \leq \lambda \sum_{i = 1}^n |a_{i1}| + \mu \sum_{i = 1}^n |a_{i2}| \leq \sum_{i = 1}^n |a_{i1}|
\]

(предполагается, что $\lambda + \mu = 1$; то же рассуждение можно проделать для произвольной суммы базисных векторов)

Замечание: $L_\infty$ на векторах переходит в максимум из $L_1$-норм строк.

Утверждение (без доказательства): для любой векторной нормы и соответствующей ей матричной нормы $||A|| \leq \max_{\lambda \in \text{собственные числа } A} \lambda$.

Перейдём к разговору о числах обусловленности. Пусть в некотором смысле $x$ — точное ("настоящее") значение, $x'$ — приближённое. Тогда мы будем называть $|x - x'|$ абсолютной погрешностью, а $\frac {|x - x'|} {|x|}$ — относительной.

Рассмотрим "самую распространённую в мире задачу" — решение системы линейных уравнений $Ax = b$. Но в компьютере эти системы представляются приближённо, и там хранится некоторое $\tilde{A} \tilde{x} = \tilde{b}$. Обозначим

\begin{align*}
\Delta A &= A - \tilde{A} \\
\Delta x &= x - \tilde{x} \\
\Delta b &= b - \tilde{b} \\
\end{align*}

Вычтем одну систему из другой:

\[
(A - \Delta A) (x - \Delta x) = b - \Delta b
\]

Преобразуем:

\[
(A - \Delta A) \Delta x = \Delta b - \Delta A x
\]

и ещё раз:

\[
A \Delta x = \Delta b - \Delta A (x - \Delta x)
\]

отсюда:

\[
\Delta x = A^{-1} (\Delta b - \Delta A (x - \Delta x))
\]

Обычно мы можем хранить матрицу с большей точностью, чем вектор, поэтому можно считать, что второе слагаемое равно нулю:

\[
\Delta x = A^{-1} \Delta b
\]

Оценим норму $\Delta x$:

\[
\frac {||\Delta x||} {||x||} \leq
||A^{-1}|| \frac {||\Delta b||} {||x||} =
||A^{-1}|| \frac {||b||} {||x||} \frac {||\Delta b||} {||b||} \leq
||A^{-1}|| ||A|| \frac {||\Delta b||} {||b||}
\]

(оценка $||A|| \geq \frac {||b||} {||x||}$ следует из $||Ax|| \leq ||b||$)

Число $\kappa(A) = ||A^{-1}|| ||A||$ называется числом обусловленности. Чем оно меньше, тем лучше решаются системы.

Ещё одна формула (без доказательства):

\[
\frac {||\Delta x||} {||x||} \leq
\frac {\kappa(A)} {1 - \kappa(A) \frac {||\Delta(A)||} {||A||}} \left( \frac {||\Delta(A)||} {||A||} + \frac {||\Delta(b)||} {||b||} \right)
\]

Теперь повспоминаем разное про ортогональные матрицы.

\subsection{Ортогональные и самосопряжённые матрицы}

Определение: матрица перехода от ортонормированного базиса к ортонормированному называется ортогональной.

Матрицы можно записывать как строку столбцов или столбец строк:

\[
(a_{ij}) = (A^{(1)}, \ldots, A^{(n)}) = \left(
\begin{matrix}
A_{(1)} \\
\vdots \\
A_{(n)}
\end{matrix}\right)
\]

Свойства: $(A^{(i)}, A_{(j)}) = \delta_{ij}$, $A \cdot A^T = E$, $A^{-1} = A^T$.

Вспомним также определение сопряжённой матрицы: матрица $A^*$ называется сопряжённой к $A$, если для любых векторов $u, v$ выполняется $(Au, v) = (u, A^* v)$.

Матрица называется самосопряжённой, если $A^* = A$. Это эквивалентно симметричности: $A^T = A$.

\subsection{Матричные разложения}

\subsubsection{LU-разложение}

Вырастает из метода Гаусса. Запретим в нём смены строк местами и рассмотрим вариацию, в которой разрешено только домножение строки на константу и сложение строк. Первый тип операций эквивалентен домножению на единичную матрицу, в которой одна единица заменена на $\frac 1 {a_{ii}}$; второй тип эквивалентен домножению на единичную матрицу, в которой под единицей в одном из столбцов стоит $-a_{2i}, \ldots, -a_{ni}$. Таким образом, такой метод делает следующее:

\[
D_n L_{n-1} \ldots L_1 D_1 A = U
\]

где $D$ отвечают за первый тип, $L$ за второй тип, а $U$ — это унитреугольная матрица, то есть на диагонали единицы, а под диагональю нули. Произведение матриц $D$ и $L$ (обозначим его за $\tilde{L}$) легко инвертировать и получить LU-разложение: $A = LU$.

Таким образом, если мы знаем LU-разложение, можно свести решение системы $Ax = b$ к решению систем $Ly = b$ и $Ux = y$.

NB: LU-разложение существует не для всех матриц.

\subsubsection{LDL-разложение (также называется RDR)}

Пусть имеется LU-разложение $A = LU$. Напишем $A = LU = \tilde{L} D U$, вытащив диагональ из $L$. Рассмотрим ситуацию, где $A = A^T$.

\[
\tilde{L} D U = (\tilde{L} D U)^T = U^T D \tilde{L}^T
\]

Хочется верить, что $\tilde{L} = U^T$ и $U = \tilde{L}^T$. Проверим, обязательно ли это так.

\[
D \tilde{L}^T U^{-1} D^{-1} = (U^T)^{-1} \tilde{L}
\]

Можно заметить, что слева и справа стоят произведения разных унитреугольных матриц, которые равны единичным матрицам. Поэтому $U = (\tilde{L})^T$.

Поэтому каждую самосопряжённую матрицу, для которой существует LU-разложение, можно разложить в произведение нижней унитреугольной матрицы, диагональной матрицы и верхней унитреугольной матрицы.

\subsubsection{Choletsky decomposition}

Если в LDL-разложении $D = E$, то разложение называется "разложением Холецкого".

\subsubsection{QR-разложение}

Назовём элементарным вращением матрицу, в которой на пересечении каких-то двух строк и столбцов стоит матрица поворота:

\[
T_\phi =
\left(
\begin{matrix}
\cos \phi & \sin \phi \\
-\sin \phi & \cos \phi \\
\end{matrix}
\right)
\]

Замечание: для произвольного двумерного вектора $x = (x_1, x_2)$ существует такое $\phi$, что $T_\phi x = ||x|| (1, 0)$. Доказательство: $\cos \phi = \frac {x_1} {\sqrt{x_1^2 + x_2^2}}$, $\sin \phi = \frac {x_2} {\sqrt{x_1^2 + x_2^2}}$.

Матрица элементарного вращения ортогональна (т.к. не меняет нормы). Дальше действуем так: если взять произвольный вектор $(x_1, \ldots, x_n)$ и подействовать на $x_1, x_2$ элементарным вращением, то можно получить вектор $||(x_1, x_2)|| (1, 0, x_3, \ldots, x_n)$. Последовательностью элементарных вращений можно получить $||x|| (1, 0, \ldots, 0)$.

Берём произвольную матрицу и последовательностью элементарных вращений делаем из её первого столбца $(||a_1||, 0, \ldots, 0)$. Далее забываем про вращения, затрагивающие первую координату и повторяем с остальными столбцами. Если на последнем шаге в получающейся матрице на нижней правой позиции осталось отрицательное число, загоним его в ортогональную матрицу. Итого:

\[
\left(
\begin{smallmatrix}
1 &        &   &    \\
  & \ddots &   &    \\
  &        & 1 &    \\
  &        &   & -1 \\
\end{smallmatrix}
\right)
Q_{n - 1} \ldots Q_1 A = R
\]

Обозначаем произведение перед $A$ за $\tilde{Q}$ и получаем $A = \left(\tilde{Q}\right)^{-1} R = QR$.

\section{2017-11-02: лекция 2 по линейной алгебре}

\subsection{Дифференцирование векторов/матриц по векторам/матрицам}

Общая идея взятия производных:

\[
f(x) = f(x_0) + D|_{x=x_0} (x-x_0) + \overline{o} \left( |x-x_0| \right)
\]

% Хочется наложить такие ограничения:

% \begin{itemize}
% \item $\frac {d \text{скаляр}} {d \text{скаляр}} = \text{скаляр}$
% \end{itemize}

Попробуем выписать, что мы ожидаем от дифференцирования:

\[
f(x) =
\begin{pmatrix}
f_1(x) \\
f_2(x) \\
\vdots \\
f_n(x) \\
\end{pmatrix}
\quad \Rightarrow \quad
\frac {df} {dx} =
\begin{pmatrix}
\frac {df_1} {dx} (x) \\
\frac {df_2} {dx} (x) \\
\vdots \\
\frac {df_n} {dx} (x) \\
\end{pmatrix}
\]

\[
f(x) = f(x_1, x_2, \ldots, x_n)
\quad \Rightarrow \quad
\frac {\partial f} {\partial x} =
\begin{pmatrix}
\frac {\partial f} {\partial x_1} (x) \\
\frac {\partial f} {\partial x_2} (x) \\
\vdots \\
\frac {\partial f} {\partial x_n} (x) \\
\end{pmatrix}
\]

А что такое производная вектор-функции по вектору?

Пусть функция — это вектор-столбец и аргумент — это вектор-столбец. Тогда хочется, чтобы производная одного по другому было матрицей ($\frac {\partial f^T} {\partial x}$ или $\frac {\partial f} {\partial x^T}$). А именно, определим матричный градиент следующим образом:

\[
\nabla_x f := \frac {\partial f^T} {\partial x} =
\begin{pmatrix}
\frac {\partial f_1} {\partial x_1} & \cdots & \frac {\partial f_m} {\partial x_1} \\
\vdots & \ddots & \vdots \\
\frac {\partial f_1} {\partial x_n} & \cdots & \frac {\partial f_m} {\partial x_n} \\
\end{pmatrix}
\]

Якобиан определим как $\frac {\partial f} {\partial x^T} = (\nabla_x f)^T$.

Заметим, что в смысле исходного определения дифференцирования (вычисления линейного приращения) якобиан удобнее, поскольку

\[
f(x) = f(x_0) + \frac {\partial f} {\partial x^T} (x-x_0) + \overline{o}(||x-x_0||)
\]

Пример: вычисление $\frac {\partial a^T x} {\partial x}$. Если бы мы просто считали $\frac {\partial a^T x} {\partial x_i}$, получилось бы $a_i$. Производная по вектору-столбцу — это вектор-столбец, поэтому $\frac {\partial a^T x} {\partial x} = a$.

Пример: $\frac {\partial x^T A x} {\partial x}$. Вспоминаем, что $x^T A x = \sum_{i, k = 1}^n a_{ik} x_i x_k$. Если напрямую посчитать $\frac {\partial x^T A x} {\partial x_i}$, то получится $\sum_{i=1}^n a_{ij} x_i + \sum_{k=1}^n a_{jk} x_k$. Это сворачивается до $\frac {\partial x^T A x} {\partial x} = (A + A^T)x$. (Очевидно, если $A = A^T$, то ответ будет $2Ax$.)

Пример: $\frac {\partial Ax} {x^T}$. Это можно не считать: поскольку дифференциал — это локальное линейное приближение, это равно $A$.

Пример: пусть $F = (f_{is}), G = (g_{is})$ — это две матрицы размером $p \times q$, причём каждая из функций в них имеет $n$ аргументов. Тогда их производные — это трёхмёрные объекты (которые мы не будем выписывать). А ещё производная их линейной комбинации равна линейной комбинации производных.

Для матричных производных выполняются очевидно проверяемые свойства:

\begin{enumerate}
\item Линейность
\item $\frac {\partial (FG)} {\partial x_j} = F \frac {\partial G} {\partial x_j} + \frac {\partial F} {\partial x_j} G$
\item $\frac {\partial F^T} {\partial x_j} = \left( \frac {\partial F} {\partial x_j} \right)^T$
\end{enumerate}

Ещё несколько очевидных замечаний. Если $x$ — это вектор-столбец длины $n$, то $\frac {\partial x} {\partial x_j}$ — это вектор, в котором все нули, кроме одной единицы на $j$-й позиции. Аналогично, $\frac {\partial x} {\partial x^T} = E$, $\frac {\partial X} {\partial X_{ij}} = E_{ij}$ (матрица, в которой только одна единица, а остальные нули).

Теперь научимся дифференцировать след матрицы. Пусть $F = (f_{is})$ — матрица размера $p \times p$, где каждая функция $f_{is}$ зависит от $n$ переменных (или, иными словами, от вектора-столбца длиной $n$).

\[
\frac {\partial (\operatorname{tr} F)} {\partial x_{j}} =
\frac {\partial \sum_{i=1}^p f_{ii}} {\partial x_{j}} =
\sum_{i=1}^p \frac {\partial f_{ii}} {\partial x_j} =
\operatorname{tr} \frac {\partial F} {\partial x_j}
\]

Теперь пусть $h(x)$ — это $n$-мерная функция от $m$-мерного вектора, а $g$ — это скалярная функция от $n$-мерного вектора. Тогда можно написать композицию $f(x) = g(h(x_1, \ldots, x_m))$. Тогда можно посчитать, что

\[
\frac {\partial f} {\partial x_j} =
\sum_{i=1}^n \frac {\partial g} {\partial y_i} \frac {\partial h_i} {\partial x_j} =
\frac {\partial h^T} {\partial x_j} \frac {\partial g} {\partial y}
\]

То же самое можно записать в матричном виде: $\frac {\partial f} {\partial x} = \frac {\partial h^T} {\partial x} \frac {\partial g} {\partial y}$.

Пусть снова $H = (h_{is})$ — матрица размером $n \times r$ из функций, принимающих на вход векторы, а $g$ — это скалярная функция, принимающая матрицу размером $n \times r$. Тогда

\[
\frac {\partial f} {\partial x_j} =
\operatorname{tr} \left( \left( \frac {\partial g} {\partial Y} \right)^T \cdot \frac {\partial H} {\partial x_j} \right) =
\operatorname{tr} \left( \left( \frac {\partial H} {\partial x_j} \right)^T \cdot \frac {\partial g} {\partial Y} \right)
\]

Посчитаем теперь производную определителя. Пусть $f(X) = \operatorname{det} X$, тогда $\frac {\partial \det X} {x_{ij}} = A_{ij}$ — алгебраическое дополнение матрицы $A$. Более общó,

\[
\frac {\partial \det X} {\partial X} = (A_{ij}) =
\det X \cdot \left( X^{-1} \right)^T
\]

Ещё можно воспользоваться вычисленными только что формулами и получить

\[
\frac {\partial \det F} {\partial x_j} = \det F \cdot \operatorname{tr} \left( F^{-1} \frac {\partial F} {\partial x_j} \right)
\]

Наконец, производная обратной матрицы:

\begin{align*}
F \cdot F^{-1} &= E \\
\frac {\partial F} {\partial x_j} F^{-1} + F \frac {\partial F^{-1}} {\partial x_j} &= 0 \\
\frac {\partial F^{-1}} {\partial x_j} &= -F^{-1} \cdot \frac {\partial F} {\partial x_j} \cdot F^{-1}
\end{align*}

\section{2017-11-09: лекция 3 по линейной алгебре}

Есть официальный конспект.

\section{2017-11-09: лекция 6}

<Начали лекцию с перечисления понятий из теорвера, с которыми должны быть знакомы все: события, вероятность, независимость.>

Напишем тривиальную оценку для вероятности того, что произойдёт хотя бы одно из событий:

\[
\P \left( \bigcup_{i=1}^n A_i \right) \leq \sum_{i=1}^n \P(A_i)
\]

Если $\P(A_i) \equiv p$, то это равно $np$.

Это грубая оценка. Есть точная оценка в лице формулы включений-исключений, но она сложная. Сейчас мы докажем некоторую лемму, которая помогает в таких ситуациях.

Пусть $A, B_1, \ldots, B_n$ — некоторые события. Будем говорить, что $A$ не зависит от $B_i$ в совокупности, если для любого подмножества $B_i$

\[
\P \left( A | \bigcap_{j \in J} B_j \right) = \P(A)
\]

\textbf{Теорема} (локальная лемма Ловаса в симметричной форме).

Пусть

\begin{enumerate}
\item $A_1, \ldots, A_n$ — события
\item $\forall i \P(A_i) \leq p$, где $p \in [0, 1]$
\item каждое $A_i$ не зависит от совокупности всех остальных событий, кроме не более чем $d$ штук
\item $ep(d+1) \leq 1$ (где $e$ — это основание натурального логарифма)
\end{enumerate}

Тогда $\P \left(\bigcup_{i=1}^n A_i \right) < 1$, то есть $\P \left(\bigcap_{i=1}^n A_i \right) > 0$.

Этот результат интереснее, чем предыдущая грубая оценка, потому что $np$ может быть больше $1$.

Рассмотрим пример применения этой теоремы. Пусть дано некоторое семейство $n$-элементных подмножеств некоторого множества, $n \geq 2$. Всегда ли (для любого ли семейства) можно так покрасить все элементы этих множеств в 2 цвета так, чтобы каждое из этих множеств было неодноцветным?

Ответ, конечно, нет. Например, при $n = 2$ достаточно взять в качестве элементов семейства все рёбра (пары вершин) в треугольнике. При произвольном $n$ нужно брать полный $n$-гиперграф на $2n$ вершинах (или, что то же самое, все подмножества размером $n$ в множестве размером $2n-1$).

Пусть рассматривается семейство множеств, где каждое множество имеет размер $m \leq 2^{n-1}$. Пусть пространство элементарных событий — это раскраска объемлющего множества. Обозначим за $A_i$ событие "$M_i$ одноцветное". Тогда его вероятность — это $\frac 2 {2^n}$ (т.к. множество может быть либо целиком белым, либо целиком чёрным). Тогда пользуемся первоначальным, простым вариантом оценки:

\[
\P \left( \bigcup_{i=1}^n A_i \right) < m \cdot 2^{1-n} \leq 1
\]

Поэтому в такой ситуации обязательно найдётся раскраска, которая не красит никакое множество в один цвет.

Теперь усилим это утверждение. Снова перейдём к терминологии гиперграфа. Пусть, во-первых, в каждом ребре ровно $n$ вершин, а во-вторых, число рёбер, содержащх данную вершину, не превосходит $k$ (т.е. у каждой вершины степень не превосходит $k$). Рассмотрим ребро $M_i$. Тогда для такой ситуации выполняется $d \leq n(k-1)$ ($d$ из формулировки теоремы), т.к. каждая вершина в каждом ребре накрывается не более чем $k-1$ другими рёбрами, а всего вершин в ребре $n$. Тогда теорема применима при $e \cdot 2^{1-n}(n(k-1)+1) \leq 1$. При $k=n$ это выполняется, начиная с $n = 9$. Таким образом, мы полностью избавились от ограничения на количество множеств.

Теорема была сформулирована в "симметричной форме". Сформулируем её в общем случае. Пусть $A_1, \ldots, A_n$ — события. Назовём любой орграф с вершинами $A_1, \ldots, A_n$ орграфом зависимостей этих событий, если никакое $A_i$ \textbf{не} зависит от совокупности всех таких $A_j$, что $(A_i, A_j) \not\in E$. Иными словами, $A_i$ должно быть независимым от совокупности всех событий, из которых нет входящих рёбер в $A_i$. Таким образом, любой полный орграф является графом зависимостей, и поэтому интересны графы зависимостей, в которых как можно меньше рёбер.

Формулировка теоремы в общем случае звучит так. Пусть $G = (V, E)$ — такой орграф зависимостей, для которого найдутся числа $x_1, \ldots, x_n \in [0, 1)$, с которыми для всех $i$ выполняется

\[
\P(A_i) \leq x_i \cdot \prod_{j \colon (A_i, A_j) \in E} (1 - x_j)
\]

Тогда 

\[
\P \left( \bigcap_{i=1}^n \overline{A_i} \right) \geq \prod_{i=1}^n (1 - x_i) > 0
\]

Выведем из общей формулировки частную. Рассмотрим случаи.

\begin{enumerate}
\item $d = 0$. Тогда

\[
\P \left( \bigcap_{i=1}^n \overline{A_i} \right) = \prod_{i=1}^n \P(\overline{A_i}) = (1 - p)^n \geq \left( 1 - \frac 1 e \right)^n > 0
\]

\item $d \geq 1$. Построим орграф так: для каждого $A_i$ проведём рёбра во не более чем $d$ понятно каких вершин. Положим $x_i = \frac 1 {d + 1}$. Достаточно доказать, что $p \leq \frac 1 {d+1} \prod_{j \colon (A_i, A_j) \in E} \left( 1 - \frac 1 {d + 1} \right)$. Далее можно заметить, что для этого достаточно доказать, что $p \leq \frac 1 {d+1} \left( 1 - \frac 1 {d+1} \right)^d$, то есть $p \leq \frac 1 {d+1} \frac 1 e$, что дано.
\end{enumerate}

\section{2017-11-16: семинар про графы}

Здесь пойдёт речь в том числе о планарности графов. Опишем \textbf{критерий Понтрягина-Куратовского} для определения того, является ли граф планарным: граф не планарен iff в нём найдётся подграф, гомеоморфный $K_{3, 3}$ или $K_5$, где гомеоморфизм — это изоморфизм, в котором дополнительно разрешено заменять последовательность, состоящую из ребра, вершины степени 2 и другого ребра, на одно ребро.

\textbf{Критерий Визинга} мощнее: граф не планарен iff в нём найдётся подграф, стягиваемый к $K_{3, 3}$ или $K_5$. Под стягиваемостью подразумевается склеивание вершин, соединённых ребром, и если после такой операции образуются кратные рёбра, то эти рёбра склеиваются.

\section{2017-11-23: лекция 7: локальная лемма Ловаса}

<Начали с повторения формулировки обобщённой локальной леммы Ловаса и привели несколько примеров, произносившихся в прошлый раз.>

Докажем ЛЛЛ.

Нужно доказать, что

\[
\P \left( \bigcap_{i=1}^n \overline{A_i} \right) \geq \prod_{i=1}^n (1 - x_i)
\]

Будем преобразовывать левую часть.

\begin{multline*}
\P \left( \bigcap_{i=1}^n \overline{A_i} \right) =
\P \br{\overline{A_1}} \cdot
\P \br{\overline{A_2} | \overline{A_1}} \cdot
\P \br{\overline{A_3} | \overline{A_1} \cap \overline{A_2}} \cdot \ldots \cdot
\P \br{\overline{A_n} | \overline{A_1} \cap \ldots \cap \overline{A_{n-1}}} = \\
\br{1 - \P \br{A_1}}
\br{1 - \P \br{A_2 | \overline{A_1}}} \ldots
\br{1 - \P \br{A_n | \overline{A_1} \cap \ldots \cap \overline{A_{n-1}}}}
\end{multline*}

Теперь, если мы покажем, что

\[
\P \br{A_k | \overline{A_1} \cap \ldots \cap \overline{A_{k-1}}} \leq x_k
\]

то лемма будет доказана. В этом нам поможет (ещё одна) лемма.

\textbf{Лемма для доказательсва ЛЛЛ.}

\[
\P \br{ A_i | \cap_{j \in J} \overline{A_j}} \leq x_i \quad \forall J \subseteq \{ 1,2, \ldots, n \} \setminus \{ i \}
\]

(Очевидно, что это более сильное утверждение, чем то, что осталось доказать для ЛЛЛ.)

\textit{В доказательстве не записаны какие-то логические шаги. Если вы пытаетесь в нём разобраться без лекции, придётся их восстанавливать.}

\textbf{Доказательство.}

База. Пусть $|J| = 0$. Тогда по условию $\P \br{A_i} \leq x_i \cdot A$, и $A \leq 1$.

Переход. Зафиксировали $J$, и пусть всё доказано для размеров $J$ меньше, чем то, что зафиксировали. Разобьём $J$ на два подмножества: $J = J_1 \sqcup J_2$, $J_1 = \{ j \in J \colon (A_i, A_j) \in E \}$, $J_2 = J \setminus J_1$.

Рассмотрим случаи.

\begin{enumerate}
\item $J_1 = \emptyset$. Тогда

\[
\P \br{A_i \cap_{j \in J} \overline{A_j}} =
\P \br{A_i \cap_{j \in J_2} \overline{A_j}} = \P \br{A_i} \leq x_i
\]

Последнее неравенство выполняется в силу того, что мы рассматриваем орграф зависимостей.

\item $J_1 \neq \emptyset$. Пусть $J_1 = \{ j_1, \ldots, j_r \}$.

\begin{multline*}
\P \br{A_i \cap_{j \in J} \overline{A_j}} =
\frac {
\P \br{A_i \cap \br{ \cap_{j \in J_1} \overline{A_j}} | \cap_{j \in J_2} \overline{A_j} }
} {
\P \br{\cap_{j \in J_1} \overline{A_j} | \cap_{j \in J_2} \overline{A_j} }
} =
\frac {
\P \br{A_i | \cap_{j \in J_2} \overline{A_j} }
} {
\P \br{\cap_{j \in J_1} \overline{A_j} | \cap_{j \in J_2} \overline{A_j} }
} = \\
\frac {x_i \prod_{j \colon (A_i, A_j) \in E} (1 - x_j)} {\P \br{\cap_{j \in J_1} \overline{A_j} | \cap_{j \in J_2} \overline{A_j} }}
\end{multline*}

Теперь нужно показать, что знаменатель не меньше, чем произведение:

\[
\P \br{\cap_{j \in J_1} \overline{A_j} | \cap_{j \in J_2} \overline{A_j} } \geq
\prod_{j \colon (A_i, A_j) \in E} (1 - x_j)
\]

Но

\begin{multline*}
\P \br{\cap_{j \in J_1} \overline{A_j} | \cap_{j \in J_2} \overline{A_j} } = \\
\P \br{\overline{A_{j_1}} | \cap_{j \in J_2} \overline{A_j} } \cdot
\P \br{\overline{A_{j_2}} | \cap_{j \in J_2} \overline{A_j} \cap \overline{A_{j_1}} } \cdot \ldots \cdot
\P \br{\overline{A_{j_r}} | \cap_{j \in J_2} \overline{A_j} \cap \overline{A_{j_1}} \cap \ldots \cap \overline{A_{j_{r - 1}}} } = \\
\br{ 1 - \P \br{A_{j_1} | \cap_{j \in J_2} \overline{A_j} } }
\cdot \ldots \cdot
\br{ 1 - \P \br{A_{j_r} | \cap_{j \in J_2} \overline{A_j} \cap \overline{A_{j_1}} \cap \ldots \cap \overline{A_{j_{r - 1}}} } }
\end{multline*}

По индукционному предположению каждая из этих вероятностей не превосходит соответствующий $x_{j_k}$:

\[
\ldots \geq \br{1 - x_{j_1}} \cdot \br{1 - x_{j_2}} \cdot \ldots \cdot \br{1 - x_{j_r}}
\geq \prod_{j \colon (A_i, A_j) \in E} (1 - x_j)
\]

\end{enumerate}

\blacksquare

А теперь перейдём к теории Рамсея.

Определение: число Рамсея $R(s, t)$ — это такое наименьшее число $n$, что все графы с $n$ рёбрами либо содержат полный подграф на $s$ вершинах, либо дополнение полного подграфа на $t$ вершинах.

Вычисление точных значений и асимптотик $R(s, t)$ — это очень сложная задача. С помощью локальной леммы Ловаса можно показать, что $R(3, t) \geq \Omega \br{\frac {t^2} {\ln^2 t}}$.

Рассмотрим граф $G(n, p)$, т.е. граф из $n$ вершин, в котором вероятность существования каждого ребра равна $p$. Заведём систему событий $A_1, \ldots, A_{C^3_n}$, где $A_i$ означает "$i$-я тройка вершин образует треугольник", $\P(A_i) = p^3$. Аналогично введём $B_1, \ldots, B_{C^t_n}$, $\P(B_i) = (1 - p)^{C^2_t}$. Тогда хотелось бы показать, что

\[
\P \br{\bigcap_{i=1}^{C^3_n} \overline{A_i} \bigcap \bigcap_{j=1}^{C^t_n} \overline{B_j}} > 0.
\]

Рисуем граф на $C^3_n + C^t_n$ вершинах и проводим из каждой вершины рёбра во все вершины, от которой эта вершина зависит.

Далее, нужно ввести числа $x_i$. Пусть $C^3_n$ из этих чисел равны $x$, а остальные равны $y$. Тогда

\[
p^3 \leq x \cdot (1 - x)^{3n} (1 - y)^{C^t_n}
\]

и

\[
(1 - p)^{C^2_t} \leq y (1 - x)^{n C^2_t} (1 - y)^{C^t_n}.
\]

Теперь вспоминаем, что $R(3, t)$ должно быть больше, чем максимальное число $n$, при котором существуют такие $p$, $x$, $y$, что выполняются эти два неравенства. Утверждается, что $n = const \cdot \frac {t^2} {\ln^2 t}$, $y \approx \frac 1 {C^t_n}$.

\end{document}
